{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2f6d51-9f10-4947-a690-98ddeea74962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb11ae-7f52-4ce2-9d9b-20622b1c8917",
   "metadata": {},
   "source": [
    "# Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd567eb-b13e-4f04-9036-33df6383fb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  hidden_size: 128\n",
      "  intermediate_size: 352 (Calculated from ratio 2.67, multiple of 32)\n",
      "  hidden_act: silu\n",
      "  rms_norm_eps: 1e-05\n",
      "\n",
      "Sample Input Shape (Before FFN Block Norm):\n",
      "  input_to_ffn_block: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128  # Dimensionality of the model's hidden states\n",
    "# Intermediate size for the FFN. Often calculated based on hidden_size.\n",
    "# A common pattern is around 2.67 * hidden_size, rounded up to a multiple (e.g., 256).\n",
    "ffn_intermediate_ratio = 8 / 3\n",
    "multiple_of = 32 # Common multiple for FFN intermediate size\n",
    "intermediate_size = int(hidden_size * ffn_intermediate_ratio)\n",
    "# This line of code adjusts the intermediate_size to be a multiple of 'multiple_of'.\n",
    "# It does this by first adding 'multiple_of - 1' to 'intermediate_size', then performing integer division by 'multiple_of',\n",
    "# and finally multiplying the result by 'multiple_of'. This effectively rounds up 'intermediate_size' to the nearest multiple of 'multiple_of'.\n",
    "intermediate_size = ((intermediate_size + multiple_of - 1) // multiple_of) * multiple_of\n",
    "\n",
    "hidden_act = \"silu\" # Activation function (SiLU/Swish)\n",
    "rms_norm_eps = 1e-5 # Epsilon for RMSNorm\n",
    "ffn_bias = False # Whether to use bias in FFN linear layers\n",
    "\n",
    "# Sample Input (Represents output of Attention + Residual)\n",
    "batch_size = 2\n",
    "sequence_length = 10\n",
    "# This is the state before the post-attention LayerNorm\n",
    "input_to_ffn_block = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  hidden_size: {hidden_size}\")\n",
    "print(f\"  intermediate_size: {intermediate_size} (Calculated from ratio {ffn_intermediate_ratio:.2f}, multiple of {multiple_of})\")\n",
    "print(f\"  hidden_act: {hidden_act}\")\n",
    "print(f\"  rms_norm_eps: {rms_norm_eps}\")\n",
    "\n",
    "print(\"\\nSample Input Shape (Before FFN Block Norm):\")\n",
    "print(f\"  input_to_ffn_block: {input_to_ffn_block.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ec2cf-7bf1-4ec6-8881-ab72802aee6a",
   "metadata": {},
   "source": [
    "# Pre-Normalization (Post-Attention LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e476ae6-3099-4ee6-bd91-bfd9bbd5d3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Post-Attention RMSNorm:\n",
      "  normalized_hidden_states: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# Simplified RMSNorm Implementation\n",
    "class SimplifiedRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size)) # Learnable gain parameter\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32) # Calculate in float32 for stability\n",
    "        # Calculate variance (mean of squares) across the hidden dimension\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        # Normalize: input / sqrt(variance + epsilon)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        # Apply learnable weight and cast back to original dtype\n",
    "        return (self.weight * hidden_states).to(input_dtype)\n",
    "\n",
    "# Instantiate and apply the normalization\n",
    "post_attention_norm = SimplifiedRMSNorm(hidden_size, eps=rms_norm_eps)\n",
    "normalized_hidden_states = post_attention_norm(input_to_ffn_block)\n",
    "\n",
    "print(\"Shape after Post-Attention RMSNorm:\")\n",
    "print(f\"  normalized_hidden_states: {normalized_hidden_states.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def97cff-c14a-4e03-b9bd-a0d09d5e104b",
   "metadata": {},
   "source": [
    "# The Feed-Forward Network (MLP with Gated Linear Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a7a3d1-0001-4fb1-9072-f4819c1629c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes within FFN:\n",
      "  gate_output: torch.Size([2, 10, 352])\n",
      "  up_output: torch.Size([2, 10, 352])\n",
      "  gated_result: torch.Size([2, 10, 352])\n",
      "  ffn_output: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# Define FFN layers\n",
    "gate_proj = nn.Linear(hidden_size, intermediate_size, bias=ffn_bias)\n",
    "up_proj = nn.Linear(hidden_size, intermediate_size, bias=ffn_bias)\n",
    "down_proj = nn.Linear(intermediate_size, hidden_size, bias=ffn_bias)\n",
    "\n",
    "# Define the activation function (SiLU/Swish)\n",
    "# ACT2FN could be used here, but for simplicity, we directly use nn.SiLU\n",
    "if hidden_act == \"silu\":\n",
    "    activation_fn = nn.SiLU()\n",
    "else:\n",
    "    # Add other activations if needed, otherwise raise error\n",
    "    raise NotImplementedError(f\"Activation {hidden_act} not implemented in this example.\")\n",
    "\n",
    "# Apply the FFN layers to the *normalized* hidden states\n",
    "gate_output = gate_proj(normalized_hidden_states)\n",
    "up_output = up_proj(normalized_hidden_states)\n",
    "\n",
    "# Apply activation to the gate and perform element-wise multiplication\n",
    "activated_gate = activation_fn(gate_output)\n",
    "gated_result = activated_gate * up_output\n",
    "\n",
    "# Apply the final down projection\n",
    "ffn_output = down_proj(gated_result)\n",
    "\n",
    "print(\"\\nShapes within FFN:\")\n",
    "print(f\"  gate_output: {gate_output.shape}\") # (batch, seq_len, intermediate_size)\n",
    "print(f\"  up_output: {up_output.shape}\")     # (batch, seq_len, intermediate_size)\n",
    "print(f\"  gated_result: {gated_result.shape}\") # (batch, seq_len, intermediate_size)\n",
    "print(f\"  ffn_output: {ffn_output.shape}\")   # (batch, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2156175-f346-4532-9fb3-455dbe884e4b",
   "metadata": {},
   "source": [
    "# Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30ec6c1-522a-4efb-a0f1-cb54e5c86070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape after FFN Residual Connection:\n",
      "  final_output: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# Add the FFN output to the input of the FFN block (before normalization)\n",
    "final_output = input_to_ffn_block + ffn_output\n",
    "\n",
    "print(\"\\nShape after FFN Residual Connection:\")\n",
    "print(f\"  final_output: {final_output.shape}\") # Should be (batch, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b573bb-26ff-4207-a8a8-37244875ed4d",
   "metadata": {},
   "source": [
    "# Putting it Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712c885f-ea1f-4c4c-bbbb-5b7f03ead57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output shape from simplified FFN module (before residual): torch.Size([2, 10, 128])\n",
      "Output shape after external residual connection: torch.Size([2, 10, 128])\n",
      "Outputs are close: False\n"
     ]
    }
   ],
   "source": [
    "class SimplifiedLlama4FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "        self.hidden_act = config['hidden_act']\n",
    "        self.ffn_bias = config['ffn_bias']\n",
    "        self.rms_norm_eps = config['rms_norm_eps']\n",
    "\n",
    "        # Normalization Layer (applied before MLP)\n",
    "        self.norm = SimplifiedRMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n",
    "\n",
    "        # MLP Layers\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=self.ffn_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=self.ffn_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=self.ffn_bias)\n",
    "\n",
    "        # Activation\n",
    "        if self.hidden_act == \"silu\":\n",
    "            self.activation_fn = nn.SiLU()\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Activation {self.hidden_act} not implemented.\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # 1. Apply pre-FFN normalization\n",
    "        normalized_states = self.norm(hidden_states)\n",
    "\n",
    "        # 2. Apply MLP (SwiGLU)\n",
    "        gate = self.gate_proj(normalized_states)\n",
    "        up = self.up_proj(normalized_states)\n",
    "        down = self.down_proj(self.activation_fn(gate) * up)\n",
    "\n",
    "        # This module returns *only* the MLP output.\n",
    "        # The residual connection is applied outside.\n",
    "        return down\n",
    "\n",
    "# Instantiate and run the simplified module\n",
    "ffn_config_dict = {\n",
    "    'hidden_size': hidden_size,\n",
    "    'intermediate_size': intermediate_size,\n",
    "    'hidden_act': hidden_act,\n",
    "    'ffn_bias': ffn_bias,\n",
    "    'rms_norm_eps': rms_norm_eps,\n",
    "}\n",
    "\n",
    "simplified_ffn_module = SimplifiedLlama4FFN(ffn_config_dict)\n",
    "\n",
    "# Run forward pass using the module\n",
    "# Input is the state *before* the norm\n",
    "mlp_output_from_module = simplified_ffn_module(input_to_ffn_block)\n",
    "\n",
    "# Apply the residual connection externally\n",
    "final_output_from_module = input_to_ffn_block + mlp_output_from_module\n",
    "\n",
    "print(\"\\nOutput shape from simplified FFN module (before residual):\", mlp_output_from_module.shape)\n",
    "print(\"Output shape after external residual connection:\", final_output_from_module.shape)\n",
    "# Verify that the manual calculation matches the module output (should be very close)\n",
    "print(\"Outputs are close:\", torch.allclose(final_output, final_output_from_module, atol=1e-6)) # Still not sure why i am getting false with allclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f5c74-d0b4-424c-a1a1-a10789f4ae6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Ollama)",
   "language": "python",
   "name": "ollama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
